# this file contains functions universal to the project, used in every module
import datetime
import tldextract
import re
import html
import requests
import math
from bs4 import BeautifulSoup
from urllib import parse as urlparse

# download a webpage using BeautifulSoup
# returns soup object we can parse
def downloadPage(url):
    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14'
    try:
        request = requests.get(url,headers={'User-Agent':user_agent})
        page = request.content
        soup = BeautifulSoup(page, 'html.parser')
    except Exception as e: # couldn't download page
        print("DOWNLOAD ERROR: ",e)
        soup = None
    return soup

# Links in the RSS feed are generated by google, and the actual URL of the article is stored as the 'url' parameter in this link
# this function gets us the actual URL
def getURL(RSS_url):
    parsed = urlparse.urlparse(RSS_url)
    url = urlparse.parse_qs(parsed.query)['url'][0]
    return url

# original date is a long date/time string, raw_format is the format that the date is initially entered as
# for our purposes, we really only need date, not time - so this function extracts the date and converts it into a yyyy-mm-dd format (how MySQL stores dates)
def convertDate(orig_date,raw_format):
    convertedDate = datetime.datetime.strptime(orig_date,raw_format).strftime('%Y-%m-%d')
    return convertedDate

# parses URL to get the domain name of each article's link - the source
# one defect in handling the source is that, as of now, we don't know how to handle multiple-word sources beyond just storing it all as one string (so Fox News would just be stored as foxnews)
def getSource(url):
    ext = tldextract.extract(url)
    source = ext.domain
    return source

# 'Supreme Court' appears in the titles of the RSS feed with bold tags around them
# this function strips the HTML and returns a text-only version of the title
def cleanTitle(original_title):
    cleanr = re.compile('<.*?>')
    cleanTitle = html.unescape(re.sub(cleanr, '', original_title))
    return cleanTitle

# for use with generic scraper - takes out known 'fluff' (like advertisements and prompts to read more), attempts to strip text to the essentials
def cleanText(text):
    cleanedText = ''
    for line in text.split('\n'):
        line = line.strip()
        if line.lower() not in ["advertisement","story continued below",'']:
            cleanedText += (line + '\n\n')
    return cleanedText.strip()

# print preliminary article information
def printBasicInfo(title,url):
    print('Title:',title)
    print('URL:', url)

# checks whether an article is from a known "bad" source - usually aggregate sites, paywalled sites, or obscure sites that don't scrape well and aren't worth writing a scraper for
def isBlockedSource(url):
    blockedSources = ['law360','law','freerepublic','bloomberglaw','nakedcapitalism','independent','mentalfloss'] 
    if "howappealing.abovethelaw.com" in url or getSource(url) in blockedSources:
        print("Rejected - URL/source known to have a paywall, or does not contain full articles")
        return True
    else:
        return False

# checks whether the title of an article is already in the database, avoiding duplicates
# we only check for title and url because the likeliness of identical titles is crazy low, and it cuts down on reposts from other sites
def articleIsDuplicate(title,url,c):
    c.execute("""SELECT idArticle FROM article WHERE title = %s OR url = %s""",(title,url,))
    if c.rowcount == 0:
        return False
    else:
        print("Rejected - article already exists in the database")
        return True

# determines if a new billing cycle for the Google Cloud API has been reached
def isNewBillingCycle(c):
    now = datetime.datetime.now().date()
    newBillingDate = c.execute("""SELECT newBillingDate FROM analysisCap""")
    row = c.fetchone()
    newBillingDate = datetime.datetime.strptime(row['newBillingDate'].strftime("%Y-%m-%d"),"%Y-%m-%d").date()
    if now >= newBillingDate: # check if billing date has been passed
        return True
    else:
        return False

# resets analysisCap table in database for new month of API requests
def resetRequests(c):
    now = datetime.date.today()
    newBillingDate = (now + datetime.timedelta(days=32)).replace(day=1) # reset to the first of next month
    c.execute("UPDATE analysisCap SET newBillingDate=(%s),currentSentimentRequests=0,currentImageRequests=0",(newBillingDate,))

# in the small chance an article doesn't have a title (it does happen, rarely), title it Untitled [date] [time].
def untitledArticle():
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return "Untitled " + now

# The Google Alerts RSS feeds sometimes truncate long titles with a "..." - this function gets the full title by comparing the feed title against the scraped title
# worst-case scenario, just use the original title
def replaceTitle(originalTitle, scrapedTitle):
    split_title = originalTitle.split()
    title_no_ellipsis = ' '.join(split_title[:-1])
    if title_no_ellipsis.lower() in scrapedTitle.lower():
        print("Truncated title changed to -",scrapedTitle)
        return scrapedTitle
    else:
        return originalTitle