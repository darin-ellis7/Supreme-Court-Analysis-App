# this file contains functions universal to the project, used in every module
import datetime
import tldextract
import re
import html
import requests
from bs4 import BeautifulSoup
from urllib import parse as urlparse

# download a webpage using BeautifulSoup
# returns soup object we can parse
def downloadPage(url):
    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14'
    try:
        request = requests.get(url,headers={'User-Agent':user_agent})
        page = request.content
        soup = BeautifulSoup(page, 'html.parser')
    except Exception as e: # couldn't download page
        print("DOWNLOAD ERROR: ",e)
        soup = None
    return soup

# Links in the RSS feed are generated by google, and the actual URL of the article is stored as the 'url' parameter in this link
# this function gets us the actual URL
def getURL(RSS_url):
    parsed = urlparse.urlparse(RSS_url)
    url = urlparse.parse_qs(parsed.query)['url'][0]
    return url

# original date is a long date/time string, raw_format is the format that the date is initially entered as
# for our purposes, we really only need date, not time - so this function extracts the date and converts it into a yyyy-mm-dd format (how MySQL stores dates)
def convertDate(orig_date,raw_format):
    convertedDate = datetime.datetime.strptime(orig_date,raw_format).strftime('%Y-%m-%d')
    return convertedDate

# parses URL to get the domain name of each article's link - the source
# one defect in handling the source is that, as of now, we don't know how to handle multiple-word sources beyond just storing it all as one string (so Fox News would just be stored as foxnews)
def getSource(url):
    ext = tldextract.extract(url)
    source = ext.domain
    return source

# 'Supreme Court' appears in the titles of the RSS feed with bold tags around them
# this function strips the HTML and returns a text-only version of the title
def cleanTitle(original_title):
    cleanr = re.compile('<.*?>')
    cleanTitle = html.unescape(re.sub(cleanr, '', original_title))
    return cleanTitle

# for use with generic scraper - takes out known 'fluff' (like advertisements and prompts to read more), attempts to strip text to the essentials
def cleanText(text):
    cleanedText = ''
    for line in text.split('\n'):
        line = line.strip()
        if line.lower() not in ["advertisement","story continued below",'']:
            cleanedText += (line + '\n\n')
    return cleanedText.strip()

# print preliminary article information
def printBasicInfo(title,url):
    print('Title:',title)
    print('URL:', url)


def isBlockedSource(url):
    blockedSources = ['law360','law','freerepublic','bloomberglaw'] 
    if "howappealing.abovethelaw.com" in url or getSource(url) in blockedSources:
        print("Rejected - URL/source known to have a paywall, or does not contain full articles")
        return True
    else:
        return False

# checks whether the title of an article is already in the database, avoiding duplicates
# we only check for title because the likeliness of identical titles is crazy low, and it cuts down on reposts from other sites
def articleIsDuplicate(title,c):
    c.execute("""SELECT idArticle FROM article WHERE title = %s""",(title,))
    if c.rowcount == 0:
        return False
    else:
        print("Rejected - article already exists in the database")
        return True